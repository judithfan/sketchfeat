{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn import linear_model, datasets, neighbors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "\n",
    "%matplotlib inline\n",
    "from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import sklearn\n",
    "\n",
    "\n",
    "import embedding as emb\n",
    "reload(emb)\n",
    "from embedding import *\n",
    "\n",
    "import analysis_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in pre-computed VGG features (computed inside sketch_feature_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = ['P1','P2','P3','P4','P5','FC6','FC7']\n",
    "## load in and preprocess metadata associated with pre-computed VGG features\n",
    "meta = pd.read_csv('./data/METADATA.csv')\n",
    "meta = meta.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)\n",
    "meta['subj'] = meta['subj'].map(lambda x: x.split('_')[0])\n",
    "\n",
    "## load in vgg features (already preprocessed)\n",
    "this_layer = 'P3'\n",
    "feats = np.load('./data/FEATURES_{}.npy'.format(this_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load in pre-computed neural classifier outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in neural logistic regression predictions\n",
    "this_roi = 'V1' ## roi_list = ['V1', 'V2', 'LOC', 'IT', 'fusiform', 'parahippo', 'PRC', 'ento','hipp', 'mOFC']\n",
    "path_to_neural_feats = '/home/jefan/neurosketch/data'\n",
    "neural_logistic_timeseries = 'difference_logprobs_4way_{}_trial_num.csv'.format(this_roi)\n",
    "N = pd.read_csv(os.path.join(path_to_neural_feats,neural_logistic_timeseries))\n",
    "# N = pd.read_csv(neural_logistic_timeseries)\n",
    "\n",
    "## re-format 'sub' column to make sure they are the right length\n",
    "subs = N['sub'].values\n",
    "subs = [str(sub).zfill(7) for sub in subs]\n",
    "N['sub'] = subs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in baseline sketch features and labels to train classifier (from various layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 25000 features per sample; expecting 200704",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fdd748eface7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m## overall accuracy on sketches from our study\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 317\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 25000 features per sample; expecting 200704"
     ]
    }
   ],
   "source": [
    "### load in and extract baseline features to use to make predictions on drawing features\n",
    "reload(analysis_helpers)\n",
    "path_to_baselines = '../baseline_sketches'\n",
    "baseline_paths = sorted(analysis_helpers.list_files(path_to_baselines))\n",
    "def get_label_from_baseline_path(path):\n",
    "    return path.split('/')[-1].split('_')[0]\n",
    "def get_view_from_baseline_path(path):\n",
    "    return path.split('/')[-1].split('_')[1]\n",
    "def get_instance_from_baseline_path(path):\n",
    "    return path.split('/')[-1].split('_')[2]\n",
    "\n",
    "baseline_labels = map(get_label_from_baseline_path,baseline_paths)\n",
    "baseline_views = map(get_view_from_baseline_path,baseline_paths)\n",
    "baseline_instances = map(get_instance_from_baseline_path,baseline_paths)\n",
    "B = pd.read_csv(os.path.join(path_to_baselines,'baseline_sketches_metadata.csv'))\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "\n",
    "## re-extract baseline features if not already pre-computed    \n",
    "import embedding as emb\n",
    "reload(emb)\n",
    "from embedding import *    \n",
    "extract_baseline = 0\n",
    "for layer_num in np.arange(6,7):\n",
    "    if extract_baseline:\n",
    "        print 'Extracting {}'.format(layers[layer_num])        \n",
    "        BaselineFeatureExtractor = FeatureExtractor(baseline_paths,layer_num)\n",
    "        BaselineFeatures, BaselineLabels = BaselineFeatureExtractor.extract_feature_matrix()\n",
    "        np.save('./data/FEATURES_BASELINE_{}.npy'.format(layers[layer_num]), BaselineFeatures)\n",
    "        np.save('./data/LABELS_BASELINE.npy',baseline_labels)  \n",
    "        \n",
    "## load in baseline features for specific layer and normalize (preprocess)\n",
    "BaselineFeatures = np.load('./data/FEATURES_BASELINE_{}.npy'.format(this_layer))\n",
    "print 'Finished loading baseline features...'\n",
    "normalize_on = 1\n",
    "if normalize_on:\n",
    "    BaselineFeatures = analysis_helpers.normalize(BaselineFeatures)  \n",
    "    print 'Finished normalizing features...'    \n",
    "BaselineLabels = np.load('./data/LABELS_BASELINE.npy')\n",
    "\n",
    "## train logistic classifier on baseline sketches only\n",
    "clf = linear_model.LogisticRegression(penalty='l2',C=1,random_state=1).fit(BaselineFeatures, BaselineLabels) \n",
    "print 'Finished training baseline-sketch classifier...'    \n",
    "\n",
    "## overall accuracy on sketches from our study\n",
    "print clf.score(feats, meta.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through subjects to get object logits for each repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define list of subjects for which we have neural data on\n",
    "sub_list = np.unique(N['sub'].values)\n",
    "sub = sub_list[2]\n",
    "\n",
    "## loop through subjects\n",
    "vgg_neural_correspondence = []\n",
    "for sub in sub_list:\n",
    "\n",
    "    ## use logged probabilities\n",
    "    logged=True\n",
    "\n",
    "    ## subset group meta by trials from this individual subject\n",
    "    inds = (meta['subj']==sub)\n",
    "    _meta = meta[inds]\n",
    "    _meta = _meta.sort_values(['trial'])\n",
    "    _inds = _meta.index\n",
    "\n",
    "    ## subset skethc features and labels from this subject\n",
    "    test_feats = feats[_inds,:]\n",
    "    test_labels = _meta.label.values\n",
    "\n",
    "    ## get classification accuracy for this subject\n",
    "    print_acc = 0\n",
    "    if print_acc:\n",
    "        print 'Accuracy: {}'.format(clf.score(test_feats, test_labels))\n",
    "\n",
    "    ## add prediction probabilities to metadata matrix\n",
    "    cats = clf.classes_\n",
    "    probs = clf.predict_proba(test_feats)\n",
    "\n",
    "    ## what were trained vs. control objectss\n",
    "    trained_objs = np.unique(test_labels)\n",
    "    control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "\n",
    "    ## re-order so that it you have the two trained objects first, followed by the control objects\n",
    "    _ordering = np.argsort(np.hstack((trained_objs,control_objs))) ## e.g., [chair table bench bed] ==> [3 2 0 1]\n",
    "    ordering = np.argsort(_ordering) ## get indices that sort from alphabetical to (trained_objs, control_objs)\n",
    "    probs = clf.predict_proba(test_feats)[:,ordering] ## [table chair bed bench] \n",
    "    logprobs = np.log(clf.predict_proba(test_feats)[:,ordering])\n",
    "\n",
    "    if logged==True:\n",
    "        out = logprobs\n",
    "    else:\n",
    "        out = probs\n",
    "\n",
    "    _meta['t1_prob'] = out[:,0]\n",
    "    _meta['t2_prob'] = out[:,1]\n",
    "    _meta['c1_prob'] = out[:,2]\n",
    "    _meta['c2_prob'] = out[:,3]\n",
    "\n",
    "    ## also save out new columns in the same order \n",
    "    if logged==True:\n",
    "        probs = np.log(clf.predict_proba(test_feats))\n",
    "    else:\n",
    "        probs = clf.predict_proba(test_feats)\n",
    "    _meta['bed_prob'] = probs[:,0]\n",
    "    _meta['bench_prob'] = probs[:,1]\n",
    "    _meta['chair_prob'] = probs[:,2]\n",
    "    _meta['table_prob'] = probs[:,3]                 \n",
    "\n",
    "    ## get timecourse of target selectivity (log(t/f)) based on model \n",
    "    t,f,c = analysis_helpers.get_prob_timecourse('trial',_meta,version='4way')\n",
    "    vgg_targ_select = t-f\n",
    "\n",
    "    ## get timecourse of target selectivity (log(t/f)) based on BRAIN \n",
    "    brain_targ_select = N[(N['sub']==sub) & (N['condition']=='target-foil')]['probability'].values\n",
    "    \n",
    "    r,p = stats.pearsonr(vgg_targ_select,brain_targ_select)\n",
    "    vgg_neural_correspondence.append(r)\n",
    "\n",
    "    ## plot\n",
    "    plotting=0\n",
    "    if plotting:\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.scatter(vgg_targ_select,brain_targ_select)\n",
    "        plt.xlabel('vgg target selectivity')\n",
    "        plt.ylabel('brain target selectivity')\n",
    "        print 'Correlation between vgg and neural classifier output across trials: r = {}'.format(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save into column of dataframe\n",
    "X = pd.DataFrame([vgg_neural_correspondence])\n",
    "X = X.transpose()\n",
    "X.columns = [this_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FC7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.583867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.083179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.231681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.282958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.087425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.476470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.534358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.055378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.120833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.124483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.069512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.463352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.076587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.257904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.118750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.429040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.091357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.428557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.124212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.082579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.102832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.266011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.340602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.088613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.013511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.320029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.099046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.012110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.501503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FC7\n",
       "0   0.583867\n",
       "1  -0.083179\n",
       "2   0.047235\n",
       "3   0.230979\n",
       "4  -0.231681\n",
       "5   0.282958\n",
       "6   0.087425\n",
       "7   0.476470\n",
       "8   0.534358\n",
       "9   0.055378\n",
       "10 -0.120833\n",
       "11  0.124483\n",
       "12 -0.069512\n",
       "13  0.463352\n",
       "14 -0.076587\n",
       "15  0.257904\n",
       "16 -0.118750\n",
       "17  0.429040\n",
       "18  0.091357\n",
       "19  0.428557\n",
       "20 -0.124212\n",
       "21  0.082579\n",
       "22 -0.102832\n",
       "23  0.266011\n",
       "24  0.340602\n",
       "25 -0.088613\n",
       "26  0.013511\n",
       "27  0.320029\n",
       "28  0.099046\n",
       "29 -0.012110\n",
       "30  0.501503"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[this_layer] = vgg_neural_correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    BaselineFeatures, BaselineLabels, test_size=0.25, random_state=0)\n",
    "clf = linear_model.LogisticRegression(penalty='l2',C=1,random_state=1).fit(X_train, y_train)\n",
    "print clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feats, meta.label.values, test_size=0.25, random_state=0)\n",
    "clf = linear_model.LogisticRegression(penalty='l2',C=1,random_state=1).fit(X_train, y_train)\n",
    "print clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
